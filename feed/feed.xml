<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="pretty-atom-feed.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>Blog Title</title>
  <subtitle>This is a longer description about your blog.</subtitle>
  <link href="https://example.com/feed/feed.xml" rel="self" />
  <link href="https://example.com/" />
  <updated>2024-11-13T00:00:00Z</updated>
  <id>https://example.com/</id>
  <author>
    <name>Your Name</name>
  </author>
  <entry>
    <title>Apple Intelligence and the Adoption Chasm</title>
    <link href="https://example.com/blog/apple-diffusion/" />
    <updated>2024-11-13T00:00:00Z</updated>
    <id>https://example.com/blog/apple-diffusion/</id>
    <content type="html">&lt;p&gt;When I heard about Apple’s plans to roll out AI features in MacOS, it brought me back to the mid–’90s to the moment when companies started putting their web addresses in television commercials. I remember seeing those URLs on TV because, even at the time, I knew I was witnessing the web going mainstream. I&#39;m not ready to predict how we&#39;ll look back on the beginning of Apple Intelligence, but I did give it a try on my Mac and it sparked a few thoughts about where things stand for broader adoption of AI technology.&lt;/p&gt;
&lt;p&gt;But before we dig into Apple Intelligence, let&#39;s take a brief detour through one area of communication theory.&lt;/p&gt;
&lt;p&gt;In his 1962 book &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Diffusion_of_innovations&quot;&gt;Diffusion of Innovations&lt;/a&gt;&lt;/em&gt;, communication theorist Everett Rogers suggested that new technologies spread through culture in phases that map onto a normal distribution:&lt;/p&gt;
&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers.png&amp;width=600&amp;format=avif&amp;via=transform 600w, https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers.png&amp;width=603&amp;format=avif&amp;via=transform 603w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers.png&amp;width=600&amp;format=webp&amp;via=transform 600w, https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers.png&amp;width=603&amp;format=webp&amp;via=transform 603w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers.png&amp;width=600&amp;format=png&amp;via=transform&quot; alt=&quot;Rogers&#39; Diffusion of Innovations Curve&quot; width=&quot;603&quot; height=&quot;327&quot; srcset=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers.png&amp;width=600&amp;format=png&amp;via=transform 600w, https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers.png&amp;width=603&amp;format=png&amp;via=transform 603w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot;&gt;&lt;/picture&gt;
&lt;p&gt;According to Rogers, in the earliest days of a new technology, only a tiny sliver of &amp;quot;innovators&amp;quot; are willing to use it. (Most people haven&#39;t even heard of it at this stage.) As the technology develops, it attracts the attention of a larger group of tech-adventurous folks called “early adopters.” Some of these early adopters will discover valuable applications that attract the attention of a still-larger group of people. At some point—namely at 16% adoption if you accept Rogers&#39; idea that this all fits a normal distribution—everyday non-technical people (the “early majority”) start adopting it. At this point the technology has gone mainstream.&lt;/p&gt;
&lt;p&gt;This is all very beautiful on a conceptual level, with the symmetry and gracefulness of the bell curve implying an orderly coherence to the adoption of a new technology. (It also subtly implies that diffusion is inevitable, and that those who hesitate—the “laggards”—are morally suspect. This way of thinking, when it takes a more extreme form, is what L.M. Sacasas calls the &lt;a href=&quot;https://thefrailestthing.com/2013/03/01/borg-complex-a-primer/&quot;&gt;Borg Complex&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;But there is an interesting wrinkle to this beautiful curve. In 1989, a group of business consultants started discussing an observation they&#39;d made about the behavior of certain real-world technologies with respect to Rogers&#39; curve. They had noticed that some technologies, even those that seemed to be on the fast track to broad adoption, get stuck when they reach the transition from early adopters to early majority. Sometimes, technologies that are a hit with early adopters just never gain traction in the broader population. Other times the technology does eventually make the jump, but only on the second or third try (and sometimes under the auspices of a company other than the one that developed it).&lt;/p&gt;
&lt;p&gt;They called this point of discontinuity the “marketing chasm.“&lt;/p&gt;
&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers-chasm.png&amp;width=600&amp;format=avif&amp;via=transform 600w, https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers-chasm.png&amp;width=603&amp;format=avif&amp;via=transform 603w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers-chasm.png&amp;width=600&amp;format=webp&amp;via=transform 600w, https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers-chasm.png&amp;width=603&amp;format=webp&amp;via=transform 603w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers-chasm.png&amp;width=600&amp;format=png&amp;via=transform&quot; alt=&quot;Rogers&#39; Curve, with Marketing Chasm&quot; width=&quot;603&quot; height=&quot;327&quot; srcset=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers-chasm.png&amp;width=600&amp;format=png&amp;via=transform 600w, https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Frogers-chasm.png&amp;width=603&amp;format=png&amp;via=transform 603w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot;&gt;&lt;/picture&gt;
&lt;p&gt;It&#39;s called the “marketing” chasm because it derives from a basic principle of communication: understanding your audience. The idea is that there is often a &lt;em&gt;qualitative&lt;/em&gt; difference between what motivates early adopters and what motivates the early majority. Early adopters are drawn to new technologies &lt;em&gt;because&lt;/em&gt; they are new. They enjoy trying new things whether they work well or not. The early majority, on the other hand, is not interested in novelty for its own sake. They want tools that &lt;em&gt;work&lt;/em&gt;. They&#39;re willing to learn a few new things to get there, but they don&#39;t have nearly the patience that early adopters do for half-baked solutions.&lt;/p&gt;
&lt;p&gt;This is the danger of the chasm. A technology could have all the promise in the world, and legions of excited early adopters who want to help it spread. But if those early adopters—or, more pointedly, a company that has developed a new technology and who wants to sell it to a broader market—fail to present it in a way that resonates with the broader audience, progress will halt.&lt;/p&gt;
&lt;p&gt;It seems to me that the use of LLMs for general computing workflows&lt;a href=&quot;https://example.com/blog/apple-diffusion/#fn1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; is somewhere in the vicinity of the marketing chasm right now. While I believe that LLMs have so much latent power that there&#39;s no question &lt;em&gt;whether&lt;/em&gt; they&#39;ll make the leap to adoption by the majority market, I do think there are at least three major open questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How long will it take before just about everyone is leveraging LLMs for basic personal computing workflows?&lt;/li&gt;
&lt;li&gt;What will this look like once it happens?&lt;/li&gt;
&lt;li&gt;Who will figure out how to profit from it?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This brings us back to Apple, a company whose history we can understand better with the marketing chasm in our conceptual toolbelt. In fact, you might say that Apple has built its success on its ability to traverse the chasm and be the first to tap into lucrative mass markets for new technologies. The graphical user interface, the MP3 player, the smartphone… Apple did not originate any of these technologies. But in each case they were the first to market a product that succeeded with a mass audience.&lt;/p&gt;
&lt;p&gt;So if Apple is the expert in introducing a new technology to a mass audience, what does Apple Intelligence show us about the future of LLMs in the majority market?&lt;a href=&quot;https://example.com/blog/apple-diffusion/#fn2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first thing I noticed is that using the new AI features is still very much an opt-in experience. Just updating to MacOS 15.1 doesn&#39;t enable them; you also have to flip a master switch in the system settings. I hadn&#39;t expected this given how much attention Apple has given it in their advertising. This is a sign of caution on Apple&#39;s part: they&#39;re limiting the exposure of the technology to people who specifically seek it out. In this respect, they&#39;re just dipping their toes in the early majority audience rather than diving in.&lt;/p&gt;
&lt;p&gt;I also noticed that the AI features are quite modest in their presentation. Unlike AI features in other software I&#39;ve seen, Apple Intelligence is not flashy; it doesn&#39;t particularly draw attention to the fact that it&#39;s AI. In other words, Apple is not aiming for whiz-bang with these features. They&#39;re aiming to improve the user&#39;s ability to accomplish certain tasks through targeted application of some new capabilities. This kind of move makes sense in the transition from early adopters to early majority.&lt;/p&gt;
&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Fwriting-tools.png&amp;width=600&amp;format=avif&amp;via=transform 600w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Fwriting-tools.png&amp;width=600&amp;format=webp&amp;via=transform 600w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://example.com/.11ty/image/?src=content%2Fblog%2Fapple-diffusion%2Fwriting-tools.png&amp;width=600&amp;format=png&amp;via=transform&quot; alt=&quot;Writing Tools showing Apple Intelligence&quot; width=&quot;600&quot; height=&quot;361&quot;&gt;&lt;/picture&gt;
&lt;p&gt;The last observation I&#39;ll make is that the features are tightly focused. That&#39;s the positive way to say it; “limited” would also work. Early adopters who are used to interacting with LLMs via prompts will notice how much less freedom Apple Intelligence gives them. Instead of the wide-open field of a free-text prompt, you get a dialog box with a handful of pre-defined buttons such as “proofread,” “make friendly,” and “summarize.” This too makes sense in light of the marketing chasm: what early adopters see as powerful versatility, the early majority sees as bewildering complexity. But this isn&#39;t just a user-facing benefit. By reducing the scope of what its built-in LLM can do, Apple may be able to avoid the embarrassments that have followed the launch of branded LLMs (e.g. Gemini) that give users the freedom of motion to construct full prompts.&lt;/p&gt;
&lt;p&gt;What remains to be seen is whether this constricted user experience is just a transitional step, or if it will remain the shape of mass LLM deployment moving forward. This will be worth watching, because in the long run the most consequential interface to LLMs will not be the chatbot (where the fact that one is using an LLM is front-and-center). Rather, LLMs will have the greatest impact through the as-yet-undetermined ways in which they are woven into what we think of as everyday computing.&lt;/p&gt;
&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;
&lt;p&gt;&lt;a id=&quot;fn1&quot;&gt;&lt;/a&gt;
1: I&#39;m highlighting the use of LLMs in &amp;quot;general computing workflows&amp;quot; in contrast to the chatbot, despite the latter being by far the most widely-deployed mode for LLMs.&lt;/p&gt;
&lt;p&gt;&lt;a id=&quot;fn2&quot;&gt;&lt;/a&gt;
2: I know that Apple Intelligence is about more than LLMs, but LLMs are a substantial part of it. They&#39;re also what I&#39;m most interested in.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>AI and Disruption</title>
    <link href="https://example.com/blog/ai-disruption-meaning/" />
    <updated>2024-10-30T00:00:00Z</updated>
    <id>https://example.com/blog/ai-disruption-meaning/</id>
    <content type="html">&lt;p&gt;I distinctly remember my first encounter with a large language model (LLM). It was sometime in early 2022, and I&#39;d been hearing enough about GPT-3 (a forerunner of ChatGPT) that I wanted to try it out myself. So I took one of my recent ministry newsletters, pasted the first three-quarters of it into the prompt, and asked the model to compose a new conclusion.&lt;/p&gt;
&lt;p&gt;I was ready to be impressed if the model could bring the story to a coherent close. I was not ready for what it actually produced, which was something like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“We are truly grateful for your unwavering support that enables us to continue reaching out and making an impact on young lives like J.&#39;s. This story is a testament to God&#39;s unfailing love and pursuit of His children, even in the darkest times…”&lt;a href=&quot;https://example.com/blog/ai-disruption-meaning/#fn1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What floored me was that the model not only picked up on the concrete &lt;em&gt;events&lt;/em&gt; of the story, but also closed the letter by &lt;em&gt;casting vision&lt;/em&gt; for the kingdom-building opportunities in campus ministry. In other words, it generated a paragraph that, until that point, I would have assumed could only have been produced by an articulate human writer.&lt;/p&gt;
&lt;p&gt;On an emotional level, as I read what the model produced I experienced that sinking sensation you get in your gut when you discover that something has gone terribly wrong. I may have even said a prayer in that moment along the lines of, “Lord, have mercy.”&lt;/p&gt;
&lt;p&gt;Why that response rather than the unalloyed excitement many others have felt? To be sure, I am excited about the technical possibilities offered by LLMs and other forms of AI.&lt;a href=&quot;https://example.com/blog/ai-disruption-meaning/#fn2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; I have spent many hours experimenting with LLMs and related tools since that first foray with GPT-3, and I&#39;m convinced that there are many highly beneficial applications for these technologies. But I would also say that my “Lord, have mercy” prayer still captures my overall feeling about AI. I feel this way because I expect AI to be one of the most &lt;em&gt;disruptive&lt;/em&gt; technologies of our time. Exploring that idea—disruption—is my purpose in this post.&lt;/p&gt;
&lt;p&gt;When I say that AI will be disruptive, I mean that it will bring about changes that increasingly display two characteristics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Impact: It will force &lt;em&gt;significant&lt;/em&gt; changes to our social, cultural, and economic order.&lt;/li&gt;
&lt;li&gt;Speed: These changes will occur &lt;em&gt;quickly enough&lt;/em&gt; that people and institutions will not be able to adapt smoothly, but will experience jarring discontinuity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When both of these characteristics are present, we experience a technology as disruptive. It&#39;s what we experienced in the 80&#39;s when the personal computer made rapid inroads into businesses of all sizes and “computerization” resulted in the obsolescence of entire categories of employment. We saw it again in the late aughts and early 10&#39;s when the smartphone put the world in everyone&#39;s pocket and rapidly re-ordered norms regarding what it means to be present in-person with someone.&lt;/p&gt;
&lt;p&gt;Having cited those recent examples, let me be clear that “disruptive” is not the same as “destructive” or “morally wrong.” While it&#39;s in our nature to experience rapid, significant change negatively, this doesn&#39;t mean that disruption is always bad. In other words, I&#39;m not attempting to render a moral judgment on AI technology by labeling it “disruptive.” (It&#39;s vital that we learn to make moral judgments about AI, but it&#39;s not what I&#39;m trying to do in this particular discussion.)&lt;/p&gt;
&lt;p&gt;In subsequent posts I&#39;ll flesh out particular ways in which I expect AI will be disruptive, but for the purpose of illustrating what disruption &lt;em&gt;means&lt;/em&gt; let&#39;s consider one example of a disruption that is already well underway: students using LLMs to cheat on tests and papers.&lt;/p&gt;
&lt;p&gt;LLMs like ChatGPT have seriously undermined several important techniques that educators rely on for evaluating students&#39; learning. As a philosophy major, I had several courses in which my grade was entirely based on papers, with not a single in-class quiz or exam the entire semester. I had other classes in which take-home tests were common. My professors ran their courses this way because it gave them low-cost yet reliable assessments of how well we understood the material. It was reliable because cheating was hard: in order to cheat on a paper, for instance, you would have to lift large sections of writing from someone else&#39;s paper. This is difficult to do, especially without being detected.&lt;/p&gt;
&lt;p&gt;The LLM has upended that picture entirely. Now, cheating on a paper or take-home exam—or just about &lt;em&gt;any&lt;/em&gt; form of assessment in an online course—is trivial to do and near-impossible to detect. In the blink of an eye, several major tools in our educational system&#39;s toolbox were rendered at best problematic, perhaps even invalid.&lt;/p&gt;
&lt;p&gt;To assess the impact of this change, it&#39;s crucial to realize that this is not merely a technical or logistical concern. Students must now reckon with a temptation toward moral compromise that is much stronger than it was just a few years ago: the risk of cheating is low, the reward is high, and (perhaps even worse) the perception that “everyone else is doing it” could undermine the integrity of the entire grading system. This is not mere speculation. My organization&#39;s work is with college students, and they are telling us that cheating is a big part of their moral landscape now. This is a significant impact to arise from a technological change.&lt;/p&gt;
&lt;p&gt;As for the role speed plays in this disruption, imagine how different our situation would be if, instead of exploding onto the scene all at once in 2022, the technical capabilities of ChatGPT had appeared gradually over the course of 30 years. In this scenario:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most teachers would have experienced &lt;em&gt;as students&lt;/em&gt; the temptation toward cheating with LLMs, and could apply that experience to the decisions they make about how to grade their courses.&lt;/li&gt;
&lt;li&gt;Students would have grown up learning about LLMs, how to distinguish between proper and improper uses of them, and perhaps would even have absorbed some of the moral consensus that would have been forming regarding their use.&lt;/li&gt;
&lt;li&gt;School districts and university faculties would have formed committees to examine the challenge from LLMs and issue recommendations for changes to grading practices.&lt;/li&gt;
&lt;li&gt;Academia as a whole would have time for conventional thinking about grading to change.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As it is, none of that has happened. The fast uptake of ChatGPT after its public release meant that, for all practical purposes, the technological change happened instantly. Many (most?) educators are still assigning papers and take-home tests as if nothing had changed. (Just the other day, I spoke with a friend who is an instructor for an intro-level class at a major university. She estimated that half of her students are cheating on exams by using ChatGPT to generate answers. But she doesn&#39;t have the authority to change how the course is graded, and the professors who have the authority don&#39;t see any urgent need to change.)&lt;/p&gt;
&lt;p&gt;To really understand what I mean by disruption, however, we need to step back from this particular example to consider what the total effect of LLMs and other generative AI might be. After all, students cheating with LLMs is just one example in one segment of our society, and a relatively clear-cut example at that. But I believe that the nature of generative AI suggests that we will experience disruption in just about every area of life—and that in most cases the disruption will be harder to describe than the cheating example, but no less real.&lt;/p&gt;
&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;
&lt;p&gt;&lt;a id=&quot;fn1&quot;&gt;&lt;/a&gt;
1: I generated this example using the same newsletter I used then, but this time with an open source model running on my laptop.&lt;/p&gt;
&lt;p&gt;&lt;a id=&quot;fn2&quot;&gt;&lt;/a&gt;
2: N.B. from the Correct Terminology Department: Since the release of ChatGPT, most mainstream discourse has used the term “AI” as if it referred exclusively to forms of generative AI like LLMs and image generators. A more precise and historically accurate usage recognizes that “artificial intelligence” is a much broader discipline that was well-established within computer science by the mid-20th century. I will try to avoid use of “AI” that obscures this fact, but I am not going to be pedantic about it.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Book Review: Telling a Better Story</title>
    <link href="https://example.com/blog/telling-a-better-story/" />
    <updated>2024-09-13T00:00:00Z</updated>
    <id>https://example.com/blog/telling-a-better-story/</id>
    <content type="html">&lt;p&gt;&lt;em&gt;I wrote this review some time ago. I didn&#39;t post it because I was intending to write more, but looking it over now I think it&#39;s complete enough to share.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The ostensible purpose of a book about apologetics is to help Christians persuade non-Christians of the truth of the gospel. But the nature of the subject makes it very challenging for a writer to achieve this goal. I&#39;ve read apologetics books that feel like they were written in an ivory tower (i.e. they&#39;re too abstract). I&#39;ve also read books that feel like they were written in a fortress (i.e. they&#39;re too adversarial). I&#39;ve even read some that feel like they were written in a fortified ivory tower! Joshua D. Chatraw&#39;s 2020 book &lt;em&gt;Telling a Better Story&lt;/em&gt; is a refreshing counterexample.&lt;/p&gt;
&lt;p&gt;My strongest praise for this book is that it feels like it was written in the same coffee shop where you might sit down for a conversation with a living, breathing, non-Christian neighbor. Chatraw accomplished this by consistently urging readers to pay attention to the person we&#39;re conversing with. &amp;quot;Listening well&amp;quot; was not one chapter of the book, but a constant refrain that appeared within every chapter. He conveyed this message in a tone of earnest pastoral counsel, with gentle yet insistent reminders throughout the book.&lt;/p&gt;
&lt;p&gt;This pastoral tone dovetailed well with what you might call the structural approach of the book, which Chatraw calls &amp;quot;inside-out apologetics.&amp;quot; In order to share the gospel in a way that people will actually hear, you need to first understand what the other person believes and values. Chatraw advocates thinking about these commitments in terms of stories—the narratives people use to make sense of the world and of their lives. Once you get inside someone&#39;s story, then you have a chance to tell the story of the gospel in a way that connects with what&#39;s actually important to that person.&lt;/p&gt;
&lt;p&gt;The key to this step (and the origin of the book&#39;s title) is recognizing that the gospel does a better job of addressing the needs and desires everyone feels. Much of the substance of the book traces this out along various dimensions with different issues. Take for example the issue of expressive individualism. Many books do a great job analyzing the effects of expressive individualism in our culture, and indeed this book gives a good introductory picture of the problem. But it also points out that just about everyone, though they might live out the tenets of expressive individualism in most aspects of their life, would vehemently defend the idea that certain relationships call for self-sacrifice for the sake of the other person. Expressive individualism can&#39;t account for self-sacrifice. The gospel can.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Book Review: Essentialism by Greg McKeown</title>
    <link href="https://example.com/blog/essentialism/" />
    <updated>2022-03-07T00:00:00Z</updated>
    <id>https://example.com/blog/essentialism/</id>
    <content type="html">&lt;p&gt;It is possible to read Greg McKeown’s 2014 book &lt;em&gt;Essentialism: The Disciplined Pursuit of Less&lt;/em&gt; as a collection of techniques for time management. There is some promise to this approach, as McKeown has gathered together a number of astute observations and suggestions to help readers better manage their priorities. But overall I believe that anyone who truly wants to improve their productivity would do better to skip this book.&lt;/p&gt;
&lt;p&gt;Before I describe where I see this book falling short, I&#39;ll give it credit for its strengths. There are some good reasons that people I respect have found this book valuable enough to make it required reading in DiscipleMakers’ staff training program.&lt;/p&gt;
&lt;p&gt;The principal strength of the book is that it unflinchingly presents the inescapability of trade-offs in decision-making. As Oliver Burkeman described in &lt;a href=&quot;https://brianroberg.com/blog/four-thousand-weeks&quot;&gt;&lt;em&gt;Four Thousand Weeks,&lt;/em&gt;&lt;/a&gt; there is a tendency among productivity gurus to promise that, if only you master their technique, you will be able to achieve all your life’s ambitions without having to say “no” to any deeply-held desires. McKeown rejects this false promise, early and often. In fact, the characteristic flaw of McKeown’s “Non-Essentialist” bogeyman is his inability (or unwillingness) to admit that we can only say “yes” to one thing by saying “no” to many other things.&lt;/p&gt;
&lt;p&gt;If realism about trade-offs is the main strength of the book, it also does a good job presenting various corollary arguments. For example, McKeown convincingly describes the dangers of over-optimistic planning and makes a good case for the importance of including buffer in our schedules. He provides a useful survey of research into cognitive biases that affect our decision-making. Also, his discussion of the value of routine and habit confirmed my inclination to read further on this topic. While there were points when his manner of presentation verges on the trivial, the book undoubtedly has some solid content.&lt;/p&gt;
&lt;p&gt;That said, my overall evaluation of the book is not positive. In fact, I feel I can&#39;t write about this book  without describing the magnitude of my subjective reaction to it. I found myself viscerally disliking the book to a degree I rarely experience. While many complaints come to mind, I will focus my critique to two points: one concerning style, the other a matter of more substance.&lt;/p&gt;
&lt;p&gt;Stylistically, I found McKeown&#39;s approach to his topic to be off-puttingly pretentious. As I&#39;ve said, the book contains some solid content relating to time management and prioritization. But McKeown talks a much bigger game than that. He wants you to believe that Essentialism(TM) will transform your life and unlock a life of true fulfillment. He says this explicitly in his concluding chapter, but he works to build an aura of grandeur throughout the book. He often does this by associating his ideas with the lives of revered people, including Rosa Parks and Mohandas Gandhi. Or he draws (superficial) connections to the ideas of noted  thinkers. At one point he name-drops Aristotle and Heidegger in the same paragraph. But I found all this unconvincing. The overall impression is that of a business school student wearing a philosophy department sweatshirt.&lt;/p&gt;
&lt;p&gt;But my biggest objection with &lt;em&gt;Essentialism&lt;/em&gt; is not its pretentious lack of substance. The substance it does have is even more troubling.&lt;/p&gt;
&lt;p&gt;Notwithstanding his puffed-up presentation, McKeown does make claims about what it means to live a virtuous life. The problem is that his system of ethics is thoroughly unbiblical. The highest ethical imperative for the Essentialist is not to love God or to love other people; it is to realize one&#39;s own ambitions. The Essentialist must steadfastly reject opportunities that do not advance his own interests, period. Self-sacrificial service is moral failure. And he doesn&#39;t merely imply this. He states it explicitly, and even strengthens his case by invoking respected figures like Peter Drucker who make the same claim. The instances in which he praises people for choosing to spend time with loved ones are not counter-examples, because in these cases the Essentialist has decided that investing in those relationships is what self-actualization looks like (for now).&lt;/p&gt;
&lt;p&gt;And here is why I believe that the technical merits of &lt;em&gt;Essentialism&lt;/em&gt; do not make up for its faults: McKeown&#39;s  productivity advice and his self-centered framework are inextricably linked. While some chapters contain more self-advancement than others, this ethic is the foundation of the book. It&#39;s what gives McKeown leverage in his exhortations that we slim down our commitments. It&#39;s what allows him to propose heuristics like his &amp;quot;90% Rule&amp;quot; without getting bogged down in questions of justice for the people affected by our decisions (such as job applicants). It&#39;s how he can talk up the value of setting boundaries in relationships without troubling himself to distinguish the difference between enabling and helping. In sum, whenever the interests of another person start to impinge on a would-be Essentialist&#39;s personal agenda, he offers a license to write that person off like a bad debt.&lt;/p&gt;
&lt;p&gt;These considerations lead me to conclude that whatever assets the book may have, its liabilities far outweigh them. As I noted at the beginning, it comes closest if your aim is simply to gain some insight into techniques for time management. But if this is your goal, I suggest you would do better to go right to the source and read the &lt;em&gt;Harvard Business Review&lt;/em&gt; articles that represent the wisdom of the business school world. If you want to go deeper with a book that explores the intersection of technique and biblical motivation, I&#39;d suggest Matt Perman&#39;s &lt;em&gt;What&#39;s Best Next&lt;/em&gt;. While it&#39;s been quite a while since I read that book,  I&#39;m considering picking it up again. I suspect I may find even more value in it now by contrast with &lt;em&gt;Essentialism.&lt;/em&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Book Review: Four Thousand Weeks by Oliver Burkeman</title>
    <link href="https://example.com/blog/four-thousand-weeks/" />
    <updated>2022-01-31T00:00:00Z</updated>
    <id>https://example.com/blog/four-thousand-weeks/</id>
    <content type="html">&lt;p&gt;Every once in a while I&#39;ve looked for a book that presents a cogent and intellectually honest case for finding meaning in life, but without reference to God. My various casual searches were thwarted by the fact that the most prominent books that are tagged as “atheist” tend towards polemic. Since most real people are not polemicists, I judged such books unhelpful for understanding the views of people I might actually talk to. I’d given up looking.&lt;/p&gt;
&lt;p&gt;I was reminded of this desire as I read Oliver Burkeman&#39;s new book, &lt;em&gt;Four Thousand Weeks: Time Management for Mortals&lt;/em&gt;. As indicated by its subtitle, the nominal topic of the book is time management.  But the real scope of the book is both broader and deeper. In fact, I think a more accurate (if less marketable) subtitle would be something like &lt;em&gt;An Atheist’s Guide to Living Well.&lt;/em&gt; In the end I concluded that this book delivers not only helpful insights related to time management, but also a broader perspective toward life that I found valuable to read even though I profoundly disagree with some of Burkeman’s basic premises.&lt;/p&gt;
&lt;p&gt;Burkeman takes it as a given that there is no God—at least no God about whom we can know enough that He should figure into any of our decision-making. But Burkeman’s tone is nothing like that of the strident atheists who sell books by railing against God. In fact, both times he quotes Scripture in the book he does so with a respectful tone (even if one of the quotes was taken out of context). But it was a verse of Scripture he did &lt;em&gt;not&lt;/em&gt; quote which best captures a major part of Burkeman’s thesis. That verse is James 4:14: “Yet you do not know what tomorrow will bring. What is your life? For you are a mist that appears for a little time and then vanishes.”&lt;/p&gt;
&lt;p&gt;The principal virtue of this book is that it provides—in effect, if not by intent—a clear, thorough, and insightful exploration of the implications of James 4:14. The “four thousand weeks” of the title refers to the number of weeks (roughly) in an 80-year life span, and Burkeman does an excellent job calling out ways in which we tend to ignore the fact of our mortality—especially in the peculiar corner of the self-help publishing world that focuses on time management and productivity.&lt;/p&gt;
&lt;p&gt;Burkeman asserts that time management is a failed discipline, and not only because it doesn’t deliver on its promises in terms of objective productivity. The more fundamental problem with time management technique is the implicit promise that usually lies &lt;em&gt;behind&lt;/em&gt; the technique: that if you just master the technique, you will achieve mastery over your life as well as freedom from the nagging feeling that your life is slipping through your fingers. Burkeman counters this seductive promise with hard reality. We are going to die. We are &lt;em&gt;finite beings&lt;/em&gt; who are inescapably bound to limitations in time and opportunity. To the degree that time management promises that we can accomplish everything that’s important to us so that we’ll never have to say “no” to opportunities we’d really like to pursue—to that degree, time management is a delusion that disconnects us from reality and diminishes our lives.&lt;/p&gt;
&lt;p&gt;Burkeman is relentless in driving this home in one area of life after another. I lost track of how many times he said some variation of “This might sound bleak, but there’s freedom in acknowledging that it’s true.” I actually found this quite refreshing, as it reflects a pattern of argumentation that acknowledges that what we believe about ourselves doesn’t matter if our beliefs don’t conform to the true nature of reality. It also reflected his priorities in what he wanted to convey. He was clearly more interested in helping readers think clearly in the big picture than he was in teaching particular techniques. For example, whereas he mentioned David Allen once, he devoted several pages to discussing Martin Heidegger’s philosophy about man in relation to time.&lt;/p&gt;
&lt;p&gt;Not that the book was entirely ethereal. For instance, Burkeman suggested a litmus test for judging a particular time management technique: whether the technique helps you “neglect the right things.” Techniques are helpful insofar as they help surface the hard decisions about which opportunities to pursue and which to reject. On the other hand, he said, be wary of techniques that obscure or defer such decisions, and especially of those that deny that such decisions are necessary. (Though Burkeman did not call out many techniques by name, he did single out the “Big Rocks” illustration from Stephen Covey&#39;s book &lt;em&gt;First Things First&lt;/em&gt;, calling it a “lie.”)&lt;/p&gt;
&lt;p&gt;Burkeman is similarly incisive in handling topics such as procrastination, attention, distraction, and rest. In each of these areas he went deeper than I had thought before and made connections I hadn’t made before. His discussion on these topics is worth the time of reading the book on its own.&lt;/p&gt;
&lt;p&gt;But as I mentioned earlier, the book is not so much &lt;em&gt;about&lt;/em&gt; time management as that it uses time management as a rendezvous point for an expedition into the question of how to live life well. I will still commend Burkeman as a worthy guide in this endeavor, but with an important qualification. Christians who have a solid biblical understanding of who they are in relation to God and the world can benefit from Burkeman’s secular take on how to find meaning in life. People without a biblical grounding, however, could be led astray.&lt;/p&gt;
&lt;p&gt;In fact, if James 4:14 helps us see the virtues of this book, the following verse—James 4:15—serves as an indictment of its shortcomings. That verse says, “Instead you ought to say, ‘If the Lord wills, we will live and do this or that.’” Burkeman does not believe that God’s will should figure into our decision-making, so his guidance concerning the ultimate ground of our decision-making leads him to unbiblical conclusions. Like the Preacher of Ecclesiastes, he provides valuable insight regarding matters under the sun but has little to say about eternal matters.&lt;/p&gt;
&lt;p&gt;That said, I do want to credit Burkeman for his intellectual honesty. A less careful non-theist might assert that human life has inherent dignity and meaning, forgetting that for life to be &lt;em&gt;inherently&lt;/em&gt; meaningful it must receive that meaning from something outside itself. (In the Christian conception, every person has dignity and purpose because every person is created in the image of God.) Burkeman, in contrast, shoulders the harder work of carving out a path for a person to imbue his own life with meaning in the midst of an indifferent universe. (He does this along existentialist lines, investing tremendous weight in the sanctity of human self-determination.)&lt;/p&gt;
&lt;p&gt;In sum, I would recommend &lt;em&gt;Four Thousand Weeks&lt;/em&gt; to anyone looking to better understand the constraints we face as limited creatures. It is a rare book that earnestly seeks meaning in life while taking care not to smuggle in ideas that derive from some form of theism. Within these constraints (i.e. under the sun) it offers wisdom. In addition, its vivid picture of the struggle to construct meaning for one’s own life could also help Christians as they share with their secular friends how the gospel expands the horizon of our lives.&lt;/p&gt;
</content>
  </entry>
</feed>